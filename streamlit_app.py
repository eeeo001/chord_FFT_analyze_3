import streamlit as st
import numpy as np
import librosa
import matplotlib.pyplot as plt
from scipy.fft import fft, fftfreq
from scipy.signal import find_peaks
from collections import defaultdict
import io
from audiorecorder import audiorecorder 


# Define constants
TOLERANCE = 0.015 
MIN_FREQ_HZ = 50 
MAX_FREQ_HZ = 2000 
MAX_HARMONIC_N = 8 

note_names = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']

chord_templates = {
    'Major 13th': [0, 4, 7, 11, 2, 5, 9], 'Minor 13th': [0, 3, 7, 10, 2, 5, 9], 'Dominant 13th': [0, 4, 7, 10, 2, 5, 9],
    'Major 11th': [0, 4, 7, 11, 2, 5], 'Minor 11th': [0, 3, 7, 10, 2, 5], 'Dominant 11th': [0, 4, 7, 10, 2, 5],
    'Major 9th': [0, 4, 7, 11, 2], 'Minor 9th': [0, 3, 7, 10, 2], 'Dominant 9th': [0, 4, 7, 10, 2],
    'Major 7th': [0, 4, 7, 11], 'Minor 7th': [0, 3, 7, 10], 'Dominant 7th': [0, 4, 7, 10],
    'Major': [0, 4, 7], 'Minor': [0, 3, 7]
}

# (1) frequency to MIDI note
def freq_to_midi(frequency):
    if frequency <= 0: return -1
    midi_note = 69 + 12 * np.log2(frequency / 440.0)
    return int(max(0, min(127, round(midi_note))))

# (1-2) í™”ìŒì˜ êµ¬ì„±ìŒì„ ë¬¸ìì—´ë¡œ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
def get_chord_interval_string(root_index, chord_type):
    template = chord_templates.get(chord_type)
    if not template: return ""
    notes = [note_names[(root_index + interval) % 12] for interval in template]
    return f"({', '.join(notes)})"

# (2) Chord Recommendation Logic
def get_recommended_chords(root_midi_index, chord_type):
    recommended = []
    if 'Major' in chord_type or 'Minor' in chord_type:
        dominant_root = (root_midi_index + 7) % 12
        recommended.append(f"{note_names[dominant_root]} Dominant 7th (V7)")
        subdominant_root = (root_midi_index + 5) % 12
        recommended.append(f"{note_names[subdominant_root]} Major (IV)")
        relative_minor_root = (root_midi_index + 9) % 12
        recommended.append(f"{note_names[relative_minor_root]} Minor (vi)")
    elif 'Dominant' in chord_type:
        tonic_root = (root_midi_index - 7 + 12) % 12
        recommended.append(f"{note_names[tonic_root]} Major (I)")
        subdominant_root = (tonic_root + 2) % 12
        recommended.append(f"{note_names[subdominant_root]} Minor (ii)")
    return list(set(recommended))[:4]

# (3) Core Analysis Function
def run_analysis(y, sr, source_name="Uploaded Audio"):
    # --- Display File Information ---
    st.success("File successfully loaded!")
    col1, col2 = st.columns(2)
    with col1: st.metric("Sampling Rate (sr)", f"{sr} Hz")
    with col2: st.metric("Duration", f"{len(y)/sr:.2f} seconds")

    # --- 4. Perform FFT and Calculate Spectrum ---
    N = len(y)
    yf = fft(y)
    xf = fftfreq(N, 1/sr)
    half_n = N // 2
    xf_positive = xf[:half_n] 
    yf_positive = np.abs(yf[:half_n]) 
    
    st.subheader("Frequency Spectrum Visualization")
    # --- 5. Visualize Spectrum ---
    fig, ax = plt.subplots(figsize=(10, 4))
    ax.plot(xf_positive, yf_positive)
    ax.set_title(f'Frequency Spectrum: {source_name}')
    ax.set_xlabel('Frequency (Hz)')
    ax.set_ylabel('Magnitude')
    ax.set_xlim([MIN_FREQ_HZ, MAX_FREQ_HZ]) 
    ax.grid(True)
    st.pyplot(fig)
    


    # --- 6. Peak Identification and Harmonic Filtering (Core Logic) ---
    magnitude_threshold = np.max(yf_positive) * 0.05
    peak_indices, _ = find_peaks(yf_positive, height=magnitude_threshold, prominence=magnitude_threshold * 0.3) 
    valid_indices = [i for i in peak_indices if MIN_FREQ_HZ <= xf_positive[i] <= MAX_FREQ_HZ]
    peak_frequencies = xf_positive[valid_indices]
    peak_magnitudes = yf_positive[valid_indices]

    initial_sorted_peaks = sorted(zip(peak_frequencies, peak_magnitudes), key=lambda x: x[1], reverse=True)
    filtered_fundamentals = []
    
    for freq, mag in initial_sorted_peaks:
        is_harmonic = False
        for existing_freq, _ in filtered_fundamentals:
            for n in range(2, MAX_HARMONIC_N + 1):
                expected_harmonic_freq = existing_freq * n
                if abs(freq - expected_harmonic_freq) / expected_harmonic_freq < TOLERANCE:
                    is_harmonic = True
                    break
            if is_harmonic: break
        if not is_harmonic: filtered_fundamentals.append((freq, mag))

    filtered_fundamentals.sort(key=lambda x: x[0])
    fundamental_frequencies = [f for f, m in filtered_fundamentals]
    fundamental_midi_notes = [freq_to_midi(f) for f in fundamental_frequencies if f >= MIN_FREQ_HZ]

    st.subheader("Fundamental Frequency Analysis")
    st.markdown(f"**Detected Fundamental Frequencies (Hz):** `{np.round(fundamental_frequencies, 2)}`")
    
    # --- 7. Chord Identification (Normalized Score) ---
    best_match_score = -1.0 
    best_root_midi = -1
    best_chord_type = ""
    unique_fundamental_midi_notes = sorted(list(set(note % 12 for note in fundamental_midi_notes)))
    
    for root_midi_interval in unique_fundamental_midi_notes:
        observed_intervals = set(unique_fundamental_midi_notes)
        for chord_type, template_intervals in chord_templates.items():
            expected_notes = set((root_midi_interval + interval) % 12 for interval in template_intervals)
            match_count = sum(1 for note in expected_notes if note in observed_intervals)
            template_length = len(template_intervals)
            normalized_score = match_count / template_length
            
            if match_count >= 2 and normalized_score > best_match_score:
                best_match_score = normalized_score
                best_root_midi = root_midi_interval
                best_chord_type = chord_type

    # Final Results
    if best_root_midi != -1 and best_match_score >= 0.5:
        root_name = note_names[best_root_midi]
        identified_chord = f"**{root_name} {best_chord_type}**"
        
        st.markdown(f"## ìµœì¢… ì‹ë³„ í™”ìŒ: {identified_chord}")
        st.info(f"í™”ìŒ ì¼ì¹˜ìœ¨: **{best_match_score:.2f}** (ìµœì†Œ 0.50 ì´ìƒ í•„ìš”)")
        
        recommended_chords = get_recommended_chords(best_root_midi, best_chord_type)
        if recommended_chords:
            st.subheader("Recommended Chords (ìŒì•… ì´ë¡  ê¸°ë°˜)")
            formatted_list = []
            for chord in recommended_chords:
                chord_name = chord.split("(")[0].strip()
                chord_type = " ".join(chord_name.split(" ")[1:])
                root_index = note_names.index(chord_name.split(" ")[0])
                interval_string = get_chord_interval_string(root_index, chord_type)
                formatted_list.append(f"* **{chord}** {interval_string}")
            st.markdown("\n".join(formatted_list))

    else:
        st.error("Chord identification failed. (ì¼ì¹˜ìœ¨ 50% ë¯¸ë§Œ) Please try again with a single, clear chord.")


# ----------------------------------------------------------------------
# --- Streamlit ì›¹ í˜ì´ì§€ ë ˆì´ì•„ì›ƒ ì‹œì‘ ---
# ----------------------------------------------------------------------

st.set_page_config(layout="wide")
st.title("FFT-based Chord Analyzer (í™”ìŒ ì¼ì¹˜ìœ¨ ì •ê·œí™” ì ìš©)")
st.markdown("ë¼ì´ë¸Œ ë…¹ìŒ ë˜ëŠ” íŒŒì¼ ì—…ë¡œë“œë¥¼ í†µí•´ í™”ìŒì„ ë¶„ì„í•©ë‹ˆë‹¤.")

# ----------------------------------------------------------------------
# 1. ë§ˆì´í¬ ë…¹ìŒ ì„¹ì…˜
# ----------------------------------------------------------------------
st.header("1. Analyze with Microphone ğŸ™ï¸")
st.caption("ë…¹ìŒ ì‹œì‘ ë²„íŠ¼ì„ ëˆ„ë¥´ê³  ëª…ë£Œí•˜ê²Œ í™”ìŒì„ ì—°ì£¼í•´ì£¼ì„¸ìš”.")

wav_audio_data = audiorecorder("ë…¹ìŒ ì‹œì‘", "ë…¹ìŒ ì¤‘ì§€")

if wav_audio_data is not None and len(wav_audio_data) > 5000:
    st.info("Audio detected. Starting analysis using Librosa...")

    try:
        # ğŸš¨ ì¤‘ìš”: wav_audio_dataì˜ íƒ€ì…ì´ bytesì¸ì§€ í™•ì¸í•˜ê³  ì˜¤ë¥˜ ì¶œë ¥
        if not isinstance(wav_audio_data, bytes):
            st.error(f"FATAL ERROR: `audiorecorder` returned an unexpected object type.")
            st.code(f"Expected type: <class 'bytes'>, Actual type: {type(wav_audio_data)}")
            # ë§Œì•½ AudioSegmentê°€ ëœ¬ë‹¤ë©´, Streamlit ìºì‹œë‚˜ ì¬ì‹œì‘ ë¬¸ì œì¼ ê°€ëŠ¥ì„±ì´ 99%
            st.stop()


        # WAV ë°”ì´íŠ¸ ë°ì´í„°ë¥¼ io.BytesIO ê°ì²´ë¡œ ê°ì‹¸ librosaì— ì „ë‹¬
        audio_io = io.BytesIO(wav_audio_data)
        audio_io.seek(0) 

        # librosa ë¡œë“œ ë° ì •ê·œí™”
        y, sr = librosa.load(audio_io, sr=None) 
        if np.max(np.abs(y)) > 0:
            y /= np.max(np.abs(y))
        
        # ë¶„ì„ ì‹¤í–‰
        run_analysis(y, sr, "Recorded Audio")

    except Exception as e:
        st.error(f"Failed to process the recorded audio: {e}")
        st.caption("ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì¤‘ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. í™˜ê²½ì„¤ì •(FFmpeg ë“±)ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

else:
    st.write("No audio has been recorded yet.")


# ----------------------------------------------------------------------
# 2. íŒŒì¼ ì—…ë¡œë“œ ì„¹ì…˜
# ----------------------------------------------------------------------
st.header("---")
st.header("2. Analyze from File Upload ğŸ“")

uploaded_file = st.file_uploader("Select an Audio File (WAV, MP3 recommended)", type=['wav', 'mp3'], key='uploader')

if uploaded_file is not None:
    st.info("File detected. Starting analysis...")

    try:
        y, sr = librosa.load(uploaded_file, sr=None)
        run_analysis(y, sr, uploaded_file.name)

    except Exception as e:
        st.error(f"Error: Failed to analyze audio file.: {e}")
        st.info("Please check if the file is a supported format (WAV or MP3) and retry.")
