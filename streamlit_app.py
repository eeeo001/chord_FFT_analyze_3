import streamlit as st
import numpy as np
import librosa
import matplotlib.pyplot as plt
from scipy.fft import fft, fftfreq
from scipy.signal import find_peaks
from collections import defaultdict
import io
from pydub import AudioSegment
# ë…¹ìŒ ê¸°ëŠ¥ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€ (pip install audiorecorder pydub)
from audiorecorder import audiorecorder 
# Note: pydub requires FFmpeg to be installed on the system.


# Define constants
TOLERANCE = 0.015 # 1.5% Tolerance for harmonic check
MIN_FREQ_HZ = 50 # Filter out noise below 50Hz
MAX_FREQ_HZ = 2000 # Max frequency for spectrum visualization
MAX_HARMONIC_N = 8 # Maximum harmonic check range

# Note names for output
note_names = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']

# Chord Templates (Optimized for up to 7 Notes)
# í…œí”Œë¦¿ì˜ ê¸¸ì´ê°€ ê¸¸ìˆ˜ë¡, ì‹¤ì œ ê´€ì°°ëœ ìŒì´ ë¶€ì¡±í•  ë•Œ ì •ê·œí™” ì ìˆ˜ê°€ ë‚®ì•„ì§.
chord_templates = {
    # 7ìŒ í™”ìŒ (7 Notes) - 13th: [1, 3, 5, 7, 9, 11, 13]
    'Major 13th': [0, 4, 7, 11, 2, 5, 9], 
    'Minor 13th': [0, 3, 7, 10, 2, 5, 9],
    'Dominant 13th': [0, 4, 7, 10, 2, 5, 9],

    # 6ìŒ í™”ìŒ (6 Notes) - 11th: [1, 3, 5, 7, 9, 11]
    'Major 11th': [0, 4, 7, 11, 2, 5],
    'Minor 11th': [0, 3, 7, 10, 2, 5],
    'Dominant 11th': [0, 4, 7, 10, 2, 5],

    # 5ìŒ í™”ìŒ (5 Notes) - 9th: [1, 3, 5, 7, 9]
    'Major 9th': [0, 4, 7, 11, 2], 
    'Minor 9th': [0, 3, 7, 10, 2],
    'Dominant 9th': [0, 4, 7, 10, 2],

    # 4ìŒ í™”ìŒ (4 Notes) - 7th: [1, 3, 5, 7]
    'Major 7th': [0, 4, 7, 11],
    'Minor 7th': [0, 3, 7, 10],
    'Dominant 7th': [0, 4, 7, 10],

    # 3ìŒ í™”ìŒ (3 Notes) - Triads: [1, 3, 5]
    'Major': [0, 4, 7],
    'Minor': [0, 3, 7]
}

# ----------------------------------------------------------------------
# --- í•¨ìˆ˜ ì •ì˜ ì‹œì‘ ---
# ----------------------------------------------------------------------

# (1) frequency to MIDI note
def freq_to_midi(frequency):
    """
    frquency(Hz) to MIDI note number (A4=440Hz, MIDI 69).
    """
    if frequency <= 0:
        return -1
    midi_note = 69 + 12 * np.log2(frequency / 440.0)
    return int(max(0, min(127, round(midi_note))))

# (1-2) í™”ìŒì˜ êµ¬ì„±ìŒì„ ë¬¸ìì—´ë¡œ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
def get_chord_interval_string(root_index, chord_type):
    template = chord_templates.get(chord_type)
    if not template:
        return ""
    
    notes = [note_names[(root_index + interval) % 12] for interval in template]
    return f"({', '.join(notes)})"


# (2) Chord Recommendation Logic (ì¶”ì²œ ì½”ë“œ ë¡œì§)
def get_recommended_chords(root_midi_index, chord_type):
    recommended = []
    
    # 1. Tonic (I/i) ì½”ë“œì¼ ë•Œ (Major, Minor, 7th, 9th, 11th, 13th ëª¨ë‘ í•´ë‹¹)
    if 'Major' in chord_type or 'Minor' in chord_type:
        
        # V7 (Dominant) ì½”ë“œ ì¶”ì²œ: ë£¨íŠ¸ì—ì„œ +7
        dominant_root = (root_midi_index + 7) % 12
        dominant_name = note_names[dominant_root]
        recommended.append(f"{dominant_name} Dominant 7th (V7)")
        
        # IV (Subdominant) ì½”ë“œ ì¶”ì²œ: ë£¨íŠ¸ì—ì„œ +5
        subdominant_root = (root_midi_index + 5) % 12
        subdominant_name = note_names[subdominant_root]
        recommended.append(f"{subdominant_name} Major (IV)")
        
        # vi (Relative Minor/Tonic Substitute) ì½”ë“œ ì¶”ì²œ: ë£¨íŠ¸ì—ì„œ +9
        relative_minor_root = (root_midi_index + 9) % 12
        relative_minor_name = note_names[relative_minor_root]
        recommended.append(f"{relative_minor_name} Minor (vi)")

    # 2. Dominant (V7, V9, V11, V13) ì½”ë“œì¼ ë•Œ
    elif 'Dominant' in chord_type:
        # I (Tonic) ì½”ë“œ ì¶”ì²œ: ë£¨íŠ¸ì—ì„œ -7 (í•´ê²°)
        tonic_root = (root_midi_index - 7 + 12) % 12
        tonic_name = note_names[tonic_root]
        recommended.append(f"{tonic_name} Major (I)")
        
        # ii (Minor Subdominant) ì½”ë“œ ì¶”ì²œ: Iì—ì„œ +2
        subdominant_root = (tonic_root + 2) % 12
        subdominant_name = note_names[subdominant_root]
        recommended.append(f"{subdominant_name} Minor (ii)")
    
    return list(set(recommended))[:4]


# (3) Core Analysis Function (ê¸°ì¡´ ì½”ë“œë¥¼ í•¨ìˆ˜ë¡œ í†µí•©)
def run_analysis(y, sr, source_name="Uploaded Audio"):
    # --- Display File Information ---
    st.success("File successfully loaded!")
    col1, col2 = st.columns(2)
    with col1:
        st.metric("Sampling Rate (sr)", f"{sr} Hz")
    with col2:
        st.metric("Duration", f"{len(y)/sr:.2f} seconds")

    # ----------------------------------------------------------------------
    # --- 4. Perform FFT and Calculate Spectrum ---
    # ----------------------------------------------------------------------
    N = len(y)
    yf = fft(y)
    xf = fftfreq(N, 1/sr)
    
    half_n = N // 2
    xf_positive = xf[:half_n] # Positive Frequencies
    yf_positive = np.abs(yf[:half_n]) # Magnitude (Amplitude Spectrum)
    
    st.subheader("Frequency Spectrum Visualization")
    
    # --- 5. Visualize Spectrum ---
    fig, ax = plt.subplots(figsize=(10, 4))
    ax.plot(xf_positive, yf_positive)
    ax.set_title(f'Frequency Spectrum: {source_name}')
    ax.set_xlabel('Frequency (Hz)')
    ax.set_ylabel('Magnitude')
    ax.set_xlim([MIN_FREQ_HZ, MAX_FREQ_HZ]) # Musical Frequency Range
    ax.grid(True)
    st.pyplot(fig)
    # 

    # ----------------------------------------------------------------------
    # --- 6. Peak Identification and Harmonic Filtering (Core Logic) ---
    # ----------------------------------------------------------------------
    
    # 6-1. Initial Peak Identification
    magnitude_threshold = np.max(yf_positive) * 0.05
    
    # Prominence ì¡°ê±´ ê°•í™” (ìŒì„± ì‹ í˜¸ì˜ ë…¸ì´ì¦ˆ/ê³ ì¡°íŒŒ í•„í„°ë§ ê°•í™”)
    peak_indices, properties = find_peaks(yf_positive, 
                                         height=magnitude_threshold, 
                                         prominence=magnitude_threshold * 0.3) # Prominence ì„ê³„ê°’ ìƒí–¥ ì¡°ì •
    
    # Filter peaks to the musical range
    valid_indices = [i for i in peak_indices if MIN_FREQ_HZ <= xf_positive[i] <= MAX_FREQ_HZ]
    peak_frequencies = xf_positive[valid_indices]
    peak_magnitudes = yf_positive[valid_indices]

    # 6-2. Harmonic Filtering (Correcting Fundamental vs. Harmonic confusion)
    # ì§„í­ì´ í° ìˆœì„œëŒ€ë¡œ ì •ë ¬ (ê°€ì¥ í° í”¼í¬ë¶€í„° ê·¼ìŒ í›„ë³´ë¡œ ê²€ì‚¬)
    initial_sorted_peaks = sorted(zip(peak_frequencies, peak_magnitudes), key=lambda x: x[1], reverse=True)
    filtered_fundamentals = []
    
    for freq, mag in initial_sorted_peaks:
        is_harmonic = False
        
        # ì´ë¯¸ ì‹ë³„ëœ ê·¼ìŒì˜ í•˜ëª¨ë‹‰ì¸ì§€ í™•ì¸
        for existing_freq, _ in filtered_fundamentals:
            # 2ì°¨ë¶€í„° MAX_HARMONIC_Nì°¨ê¹Œì§€ í•˜ëª¨ë‹‰ ì²´í¬
            for n in range(2, MAX_HARMONIC_N + 1):
                expected_harmonic_freq = existing_freq * n
                
                # ìƒëŒ€ì  ì˜¤ì°¨ TOLERANCE ë‚´ì— ìˆëŠ”ì§€ í™•ì¸
                if abs(freq - expected_harmonic_freq) / expected_harmonic_freq < TOLERANCE:
                    is_harmonic = True
                    break
            if is_harmonic:
                break
        
        if not is_harmonic:
            # í•˜ëª¨ë‹‰ì´ ì•„ë‹ˆë©´ ê·¼ìŒìœ¼ë¡œ ì¶”ê°€
            filtered_fundamentals.append((freq, mag))

    # --- Final Fundamental List Creation ---
    filtered_fundamentals.sort(key=lambda x: x[0])
    fundamental_frequencies = [f for f, m in filtered_fundamentals]
    fundamental_midi_notes = [freq_to_midi(f) for f in fundamental_frequencies if f >= MIN_FREQ_HZ]

    st.subheader("Fundamental Frequency Analysis")
    st.markdown(f"**Detected Fundamental Frequencies (Hz):** `{np.round(fundamental_frequencies, 2)}`")
    
    # ----------------------------------------------------------------------
    # --- 7. Chord Identification (Normalized Score) ---
    # ----------------------------------------------------------------------
    
    # best_match_scoreë¥¼ ì •ê·œí™”ëœ ë¹„ìœ¨(0.0 ~ 1.0)ë¡œ ì´ˆê¸°í™”
    best_match_score = -1.0 
    best_root_midi = -1
    best_chord_type = ""
    
    # Unique Note Classes (0~11)
    unique_fundamental_midi_notes = sorted(list(set(note % 12 for note in fundamental_midi_notes)))
    
    # Match the identified notes to chord templates based on music theory.
    for root_midi_interval in unique_fundamental_midi_notes:
        observed_intervals = set(unique_fundamental_midi_notes)

        for chord_type, template_intervals in chord_templates.items():
            
            expected_notes = set((root_midi_interval + interval) % 12 for interval in template_intervals)
            
            # 1. ì¼ì¹˜í•˜ëŠ” ìŒì˜ ê°œìˆ˜
            match_count = sum(1 for note in expected_notes if note in observed_intervals)
            
            # 2. ì •ê·œí™” ì ìˆ˜ (ì¼ì¹˜ìœ¨) ê³„ì‚°
            template_length = len(template_intervals)
            normalized_score = match_count / template_length
            
            # 3. ìµœê³  ì ìˆ˜ ì—…ë°ì´íŠ¸: ì¼ì¹˜ ê°œìˆ˜ê°€ ìµœì†Œ 2ê°œ ì´ìƒì´ê³ , ì¼ì¹˜ìœ¨ì´ ë” ë†’ì„ ë•Œ ì—…ë°ì´íŠ¸
            if match_count >= 2 and normalized_score > best_match_score:
                best_match_score = normalized_score
                best_root_midi = root_midi_interval
                best_chord_type = chord_type

    # Final Results
    if best_root_midi != -1 and best_match_score >= 0.5: # ìµœì†Œ ì¼ì¹˜ìœ¨ 50% ì´ìƒìœ¼ë¡œ ê¸°ì¤€ ì„¤ì •
        root_name = note_names[best_root_midi]
        identified_chord = f"**{root_name} {best_chord_type}**"
        
        st.markdown(f"## ìµœì¢… ì‹ë³„ í™”ìŒ: {identified_chord}")
        st.info(f"í™”ìŒ ì¼ì¹˜ìœ¨: **{best_match_score:.2f}** (ìµœì†Œ 0.50 ì´ìƒ í•„ìš”)")
        
        # ì½”ë“œ ì¶”ì²œ ê¸°ëŠ¥ í˜¸ì¶œ ë° ì¶œë ¥
        recommended_chords = get_recommended_chords(best_root_midi, best_chord_type)
        
        if recommended_chords:
            st.subheader("Recommended Chords (ìŒì•… ì´ë¡  ê¸°ë°˜)")

            formatted_list = []
            for chord in recommended_chords:
                chord_name = chord.split("(")[0].strip()
                # Chord Type ì¶”ì¶œ: ì˜ˆ) Dominant 7th
                chord_type_parts = chord_name.split(" ")[1:]
                chord_type = " ".join(chord_type_parts)
                
                # Root Name ì¶”ì¶œ: ì˜ˆ) G
                root_name = chord_name.split(" ")[0]
                root_index = note_names.index(root_name)

                interval_string = get_chord_interval_string(root_index, chord_type)
                formatted_list.append(f"* **{chord}** {interval_string}")
            
            st.markdown("\n".join(formatted_list))

    else:
        st.error("Chord identification failed. (ì¼ì¹˜ìœ¨ 50% ë¯¸ë§Œ) Please try again with a single, clear chord.")


# ----------------------------------------------------------------------
# --- Streamlit ì›¹ í˜ì´ì§€ ë ˆì´ì•„ì›ƒ ì‹œì‘ ---
# ----------------------------------------------------------------------

# (2) Streamlit web page settings
st.set_page_config(layout="wide")
st.title("FFT-based Chord Analyzer (í™”ìŒ ì¼ì¹˜ìœ¨ ì •ê·œí™” ì ìš©)")
st.markdown("ë¼ì´ë¸Œ ë…¹ìŒ ë˜ëŠ” íŒŒì¼ ì—…ë¡œë“œë¥¼ í†µí•´ í™”ìŒì„ ë¶„ì„í•©ë‹ˆë‹¤.")

# ----------------------------------------------------------------------
# 1. ë§ˆì´í¬ ë…¹ìŒ ì„¹ì…˜
# ----------------------------------------------------------------------
st.header("1. Analyze with Microphone ğŸ™ï¸")
st.caption("ë…¹ìŒ ì‹œì‘ ë²„íŠ¼ì„ ëˆ„ë¥´ê³  ëª…ë£Œí•˜ê²Œ í™”ìŒì„ ì—°ì£¼í•´ì£¼ì„¸ìš”.")

# ë…¹ìŒ ì»´í¬ë„ŒíŠ¸ ìƒì„±. ë…¹ìŒëœ wav ë°”ì´íŠ¸ ë°ì´í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
wav_audio_data = audiorecorder("ë…¹ìŒ ì‹œì‘", "ë…¹ìŒ ì¤‘ì§€")

# ë°ì´í„°ê°€ ì¡´ì¬í•˜ê³  ê¸¸ì´ë„ 0ì´ ì•„ë‹ ë•Œë§Œ ì²˜ë¦¬ (ë…¹ìŒëœ ë°ì´í„°ëŠ” 5KBë³´ë‹¤ ì»¤ì•¼ ìœ íš¨í•˜ë‹¤ê³  ê°€ì •)
if wav_audio_data is not None and len(wav_audio_data) > 5000:

    st.info("Audio detected. Starting analysis...")

    try:
        # 1. WAV ë°”ì´íŠ¸ ë°ì´í„°ë¥¼ pydubì˜ AudioSegmentë¡œ ë³€í™˜
        audio_segment = AudioSegment.from_wav(io.BytesIO(wav_audio_data))

        # 2. AudioSegmentë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
        # ë°ì´í„° íƒ€ì… ì¡°ì • (pydubì€ ë³´í†µ 16ë¹„íŠ¸ ì •ìˆ˜)
        y = np.array(audio_segment.get_array_of_samples()).astype(np.float32)
        sr = audio_segment.frame_rate  # ë…¹ìŒëœ íŒŒì¼ì˜ ìƒ˜í”Œë§ ë ˆì´íŠ¸ ì‚¬ìš©

        # ì˜¤ë””ì˜¤ ì‹ í˜¸ì˜ ë³¼ë¥¨ ì •ê·œí™” (í•„ìˆ˜)
        if np.max(np.abs(y)) > 0:
            y /= np.max(np.abs(y))

        # 3. ë¶„ì„ ì‹¤í–‰
        run_analysis(y, sr, "Recorded Audio")

    except Exception as e:
        st.error(f"Failed to process the recorded audio: {e}")
        st.caption("Pydub ì˜¤ë¥˜ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‹œìŠ¤í…œì— FFmpegì´ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")

else:
    st.write("No audio has been recorded yet.")


# ----------------------------------------------------------------------
# 2. íŒŒì¼ ì—…ë¡œë“œ ì„¹ì…˜
# ----------------------------------------------------------------------
st.header("---")
st.header("2. Analyze from File Upload ğŸ“")

# (3) file uploader widget
uploaded_file = st.file_uploader("Select an Audio File (WAV, MP3 recommended)", type=['wav', 'mp3'], key='uploader')

if uploaded_file is not None:
    st.info("File detected. Starting analysis...")

    # Run analysis logic only if the file is successfully loaded.
    try:
        y, sr = librosa.load(uploaded_file, sr=None)
        
        # ë¶„ì„ ì‹¤í–‰
        run_analysis(y, sr, uploaded_file.name)

    except Exception as e:
        # When the audio file is corrupted or incorrectly formatted
        st.error(f"Error: Failed to analyze audio file.: {e}")
        st.info("Please check if the file is a supported format (WAV or MP3) and retry.")
